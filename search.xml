<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Java实现 Serializable 序列化]]></title>
    <url>%2F2019%2F07%2F21%2F1%2F</url>
    <content type="text"><![CDATA[深度理解Java实现 Serializable 序列化概念把对象转换为直接序列的过程叫对象的序列化 把字节序列恢复为对象的过程叫对象的反序列化 用途 对象持久化 跨网络数据交换，远程过程调用 对象持久化意味着一个对象的生命周期可以不取决于程序是否运行，实现序列化的对象可以生存在程序的调用之间。通过一个序列化的对象写在磁盘中，然后再调用期间恢复这个对象就可以实现对象持久化的效果 序列化可以弥补不同操作系统之间的差异，在某台电脑上创建一个的对象，将其序列化。通过网络传输就可以将其发送到另一台不同操作系统的电脑上，然后在那里准确的组装出这个对象不必关心字节的顺序或者其他细节。所以在向远程对象发送消息时，必须通过对象序列化来传输参数和返回值。 内部实现让要实现序列化的类实现java.io.Serializable接口 需要注意的是Serializable没有定义任何方法，只是一个标记接口。 对象序列化是基于字节的，因此需要OutputStream和InputStream继承层次结构。 序列化对象过程：在序列化一个对象的过程中，会创建某些OutPutStream对象，将其封装在一个ObjectOutputStream对象内，之后调用writeObject()方法序列化对象。 反序列化对象：创建某些InputStream对象，并将其封装在一个ObjectInputStream 对象内，之后调用readObject（）方法反序列化对象。 反序列化最后得到的是一个指向Object的引用，所以必须向下转型为指定类型的对象 序列化控制默认的序列化机制已经很强大了，它可以自动将对象中的所有字段自动保存和恢复，但这种默认行为有时候不是我们想要的。比如，对于有些字段，它的值可能与内存位置有关，比如默认的hashCode()方法的返回值，当恢复对象后，内存位置肯定变了，基于原内存位置的值也就没有了意义。还有一些字段，可能与当前时间有关，比如表示对象创建时的时间，保存和恢复这个字段就是不正确的。 还有一些情况，如果类中的字段表示的是类的实现细节，而非逻辑信息，那默认序列化也是不适合的。为什么不适合呢？因为序列化格式表示一种契约，应该描述类的逻辑结构，而非与实现细节相绑定，绑定实现细节将使得难以修改，破坏封装。 Java提供了多种定制序列化的机制，主要的有三种，一种是transient关键字，另外一种是实现Externalizable接口代替实现Serializable，还有一种是实现writeObject和readObject方法。 将字段声明为transient，默认序列化机制将忽略该字段，不会进行保存和恢复。比如上面的第一个实例中，假设我们在进行序列化和反序列化时不需要保存和恢复no字段的信息，那么我们可以在no字段前面加上一个transient修饰符。 1private transient String no; 运行程序我们会得到如下的结果： 1234------------------序列化前--------------[HashCode:366712642 学号:001 姓名:Ron 班级：Class 001, HashCode:1829164700 学号:002 姓名:Ron2 班级：Class 002]------------------反序列化后--------------[HashCode:2133927002 学号:null 姓名:Ron 班级：Class 001, HashCode:1836019240 学号:null 姓名:Ron2 班级：Class 002] 我们可以到no字段的内容反序列化之后变成了null。将字段声明为transient，不是说就不保存该字段了，而是告诉Java默认序列化机制，不要自动保存该字段了。 利用Serializable定义数据类型在阅读源码时，遇到一些比较抽象的方法，发现他们定义的数据类型都是用Serializable来定义，比如 123public void deleteEntriesByIDS(Serializable[] tds); public void deleteEntry(Serializable id); 这样做的原因是它可以接受多种数据类型，比如String，Integer，Long等，他们都实现了Serizlizable接口。同时JDK1,.5以后有了自动拆装箱的特征，所以均可被以上Serializable定义参数方法接受，这样做的目的就是可以节省代码，提高代码利用率。 有的方法也会利用它做返回值，同理 serialVersionUID的作用序列化的作用是将对象的状态信息转换为可存储过程或传输形式的过程。Java对象是保存在JVM的堆内存里的，也就是说如果JVM堆不存在了，那么对象也就消失。而序列化提供了一种方案是：即使JVM停机的情况下也能把对象保存下来（把Java对象序列化成可存储或传输的形式，比如保存在文件中。下次需要这个对象时就可以从文件中读取出二进制流，再从二进制流中反序列化出对象） 虚拟机是否允许反序列化，不仅取决于类路径和功能代码是否一致。还有一个非常重要的点是对比两个类的序列化ID（serialVersionUID）是否一致。 如果两个类的serialVersionUID不一致，在反序列化时就会抛出java.io.InvalidClassException，并且指出serialVersionUID不一致。 1java.io.InvalidClassException: com.hollis.User1; local class incompatible: stream classdesc serialVersionUID = 1, local class serialVersionUID = 2 这是因为，在进行反序列化时，JVM会把传来的字节流中的serialVersionUID与本地相应实体类的serialVersionUID进行比较，如果相同就认为是一致的，可以进行反序列化，否则就会出现序列化版本不一致的异常，即是InvalidCastException。 所以在《阿里巴巴Java开发手册》中才会有：在兼容性升级中，在修改类的时候，不要修改serialVersionUID的原因。除非是完全不兼容的两个版本。所以，serialVersionUID其实是验证版本一致性的。 如果一个类实现了Serializable接口，就必须手动添加一个private static final long serialVersionUID变量，并且设置初始值。如果不添加的话，系统会自己添加一个serialVersionUID，只要类稍加改变，就会重新生成，导致版本不兼容]]></content>
  </entry>
  <entry>
    <title><![CDATA[Java内存模型]]></title>
    <url>%2F2019%2F07%2F20%2F1%2F</url>
    <content type="text"><![CDATA[Java 内存模型 点此到达原文 基础并发编程的模型分类在并发编程需要处理的两个关键问题是：线程之间如何通信 和 线程之间如何同步。 通信通信 是指线程之间以何种机制来交换信息。在命令式编程中，线程之间的通信机制有两种：共享内存 和 消息传递。 在共享内存的并发模型里，线程之间共享程序的公共状态，线程之间通过写-读内存中的公共状态来隐式进行通信。 在消息传递的并发模型里，线程之间没有公共状态，线程之间必须通过明确的发送消息来显式进行通信。 同步同步 是指程序用于控制不同线程之间操作发生相对顺序的机制。 在共享内存的并发模型里，同步是显式进行的。程序员必须显式指定某个方法或某段代码需要在线程之间互斥执行。 在消息传递的并发模型里，由于消息的发送必须在消息的接收之前，因此同步是隐式进行的。 Java 的并发采用的是共享内存模型，Java 线程之间的通信总是隐式进行，整个通信过程对程序员完全透明。 Java 内存模型的抽象在 Java 中，所有实例域、静态域 和 数组元素存储在堆内存中，堆内存在线程之间共享。局部变量、方法定义参数 和 异常处理器参数 不会在线程之间共享，它们不会有内存可见性问题，也不受内存模型的影响。 Java 线程之间的通信由 Java 内存模型（JMM）控制。JMM 决定了一个线程对共享变量的写入何时对另一个线程可见。从抽象的角度来看，JMM 定义了线程与主内存之间的抽象关系：线程之间的共享变量存储在主内存中，每一个线程都有一个自己私有的本地内存，本地内存中存储了该变量以读／写共享变量的副本。本地内存是 JMM 的一个抽象概念，并不真实存在。 JMM 抽象示意图： 从上图来看，如果线程 A 和线程 B 要通信的话，要如下两个步骤： 线程 A 需要将本地内存 A 中的共享变量副本刷新到主内存去 线程 B 去主内存读取线程 A 之前已更新过的共享变量 步骤示意图： 本地内存 A 和 B 有主内存共享变量 X 的副本。假设一开始时，这三个内存中 X 的值都是 0。线程 A 正执行时，把更新后的 X 值（假设为 1）临时存放在自己的本地内存 A 中。当线程 A 和 B 需要通信时，线程 A 首先会把自己本地内存 A 中修改后的 X 值刷新到主内存去，此时主内存中的 X 值变为了 1。随后，线程 B 到主内存中读取线程 A 更新后的共享变量 X 的值，此时线程 B 的本地内存的 X 值也变成了 1。 整体来看，这两个步骤实质上是线程 A 再向线程 B 发送消息，而这个通信过程必须经过主内存。JMM 通过控制主内存与每个线程的本地内存之间的交互，来为 Java 程序员提供内存可见性保证。 重排序在执行程序时为了提高性能，编译器和处理器常常会对指令做重排序。重排序分三类： 1、编译器优化的重排序。编译器在不改变单线程程序语义的前提下，可以重新安排语句的执行顺序。 2、指令级并行的重排序。现代处理器采用了指令级并行技术来将多条指令重叠执行。如果不存在数据依赖性，处理器可以改变语句对应机器指令的执行顺序。 3、内存系统的重排序。由于处理器使用缓存和读／写缓冲区，这使得加载和存储操作看上去可能是在乱序执行。 从 Java 源代码到最终实际执行的指令序列，会分别经历下面三种重排序： 上面的这些重排序都可能导致多线程程序出现内存可见性问题。对于编译器，JMM 的编译器重排序规则会禁止特定类型的编译器重排序（不是所有的编译器重排序都要禁止）。对于处理器重排序，JMM 的处理器重排序规则会要求 Java 编译器在生成指令序列时，插入特定类型的内存屏障指令，通过内存屏障指令来禁止特定类型的处理器重排序（不是所有的处理器重排序都要禁止）。 JMM 属于语言级的内存模型，它确保在不同的编译器和不同的处理器平台之上，通过禁止特定类型的编译器重排序和处理器重排序，为程序员提供一致的内存可见性保证。 处理器重排序现代的处理器使用写缓冲区来临时保存向内存写入的数据。写缓冲区可以保证指令流水线持续运行，它可以避免由于处理器停顿下来等待向内存写入数据而产生的延迟。同时，通过以批处理的方式刷新写缓冲区，以及合并写缓冲区中对同一内存地址的多次写，可以减少对内存总线的占用。虽然写缓冲区有这么多好处，但每个处理器上的写缓冲区，仅仅对它所在的处理器可见。这个特性会对内存操作的执行顺序产生重要的影响：处理器对内存的读/写操作的执行顺序，不一定与内存实际发生的读/写操作顺序一致！ 假设处理器A和处理器B按程序的顺序并行执行内存访问，最终却可能得到 x = y = 0。具体的原因如下图所示： 处理器 A 和 B 同时把共享变量写入在写缓冲区中（A1、B1），然后再从内存中读取另一个共享变量（A2、B2），最后才把自己写缓冲区中保存的脏数据刷新到内存中（A3、B3）。当以这种时序执行时，程序就可以得到 x = y = 0 的结果。 从内存操作实际发生的顺序来看，直到处理器 A 执行 A3 来刷新自己的写缓存区，写操作 A1 才算真正执行了。虽然处理器 A 执行内存操作的顺序为：A1 -&gt; A2，但内存操作实际发生的顺序却是：A2 -&gt; A1。此时，处理器 A 的内存操作顺序被重排序了。 这里的关键是，由于写缓冲区仅对自己的处理器可见，它会导致处理器执行内存操作的顺序可能会与内存实际的操作执行顺序不一致。由于现代的处理器都会使用写缓冲区，因此现代的处理器都会允许对写-读操作重排序。 内存屏障指令为了保证内存可见性，Java 编译器在生成指令序列的适当位置会插入内存屏障指令来禁止特定类型的处理器重排序。JMM 把内存屏障指令分为下列四类： happens-beforeJSR-133 内存模型使用 happens-before 的概念来阐述操作之间的内存可见性。在 JMM 中，如果一个操作执行的结果需要对另一个操作可见，那么这两个操作之间必须要存在 happens-before 关系。这里提到的两个操作既可以是在一个线程之内，也可以是在不同线程之间。 与程序员密切相关的 happens-before 规则如下： 程序顺序规则：一个线程中的每个操作，happens-before 于该线程中的任意后续操作。 监视器锁规则：对一个监视器的解锁，happens-before 于随后对这个监视器的加锁。 volatile 变量规则：对一个 volatile 域的写，happens-before 于任意后续对这个 volatile 域的读。 传递性：如果 A happens-before B，且 B happens-before C，那么 A happens-before C。 注意，两个操作之间具有 happens-before 关系，并不意味着前一个操作必须要在后一个操作之前执行！happens-before 仅仅要求前一个操作（执行的结果）对后一个操作可见，且前一个操作按顺序排在第二个操作之前（the first is visible to and ordered before the second）。 happens-before 与 JMM 的关系如下图所示： 如上图所示，一个 happens-before 规则对应于一个或多个编译器和处理器重排序规则。 数据依赖性如果两个操作访问同一个变量，且这两个操作中有一个为写操作，此时这两个操作之间就存在数据依赖性。数据依赖分下列三种类型： 名称 代码示例 说明 写后读 a = 1; b = a; 写一个变量之后，再读这个位置。 写后写 a = 1; a = 2; 写一个变量之后，再写这个变量。 读后写 a = b; b = 1; 读一个变量之后，再写这个变量。 上面三种情况，只要重排序两个操作的执行顺序，程序的执行结果将会被改变。 前面提到过，编译器和处理器可能会对操作做重排序。编译器和处理器在重排序时，会遵守数据依赖性，编译器和处理器不会改变存在数据依赖关系的两个操作的执行顺序。 注意，这里所说的数据依赖性仅针对单个处理器中执行的指令序列和单个线程中执行的操作，不同处理器之间和不同线程之间的数据依赖性不被编译器和处理器考虑。 as-if-serial 语义 as-if-serial 语义的意思指：不管怎么重排序（编译器和处理器为了提高并行度），（单线程）程序的执行结果不能被改变。编译器，runtime 和处理器都必须遵守 as-if-serial 语义。 为了遵守 as-if-serial 编译器和处理器不会对存在数据依赖关系的操作做重排序，因为这种重排序会改变执行结果。但是如果操作之间没有数据依赖关系，这些操作就可能被编译器和处理器重排序。 举个例子： 123double pi = 3.14; //Adouble r = 1.0; //Bdouble area = pi * r * r; //C 上面三个操作的数据依赖关系如下图所示： 如上图所示，A 和 C 之间存在数据依赖关系，同时 B 和 C 之间也存在数据依赖关系。因此在最终执行的指令序列中，C 不能被重排序到 A 和 B 的前面（C 排到 A 和 B 的前面，程序的结果将会被改变）。但 A 和 B 之间没有数据依赖关系，编译器和处理器可以重排序 A 和 B 之间的执行顺序。下图是该程序的两种执行顺序： 在计算机中，软件技术和硬件技术有一个共同的目标：在不改变程序执行结果的前提下，尽可能的开发并行度。编译器和处理器遵从这一目标，从 happens-before 的定义我们可以看出，JMM 同样遵从这一目标。 重排序对多线程的影响举例： 123456789101112131415class Demo &#123; int a = 0; boolean flag = false; public void write() &#123; a = 1; //1 flag = true; //2 &#125; public void read() &#123; if(flag) &#123; //3 int i = a * a; //4 &#125; &#125;&#125; 由于操作 1 和 2 没有数据依赖关系，编译器和处理器可以对这两个操作重排序；操作 3 和操作 4 没有数据依赖关系，编译器和处理器也可以对这两个操作重排序。 1、当操作 1 和操作 2 重排序时，可能会产生什么效果？ 如上图所示，操作 1 和操作 2 做了重排序。程序执行时，线程 A 首先写标记变量 flag，随后线程 B 读这个变量。由于条件判断为真，线程 B 将读取变量 a。此时，变量 a 还根本没有被线程 A 写入，在这里多线程程序的语义被重排序破坏了！ 2、当操作 3 和操作 4 重排序时会产生什么效果（借助这个重排序，可以顺便说明控制依赖性）。 在程序中，操作 3 和操作 4 存在控制依赖关系。当代码中存在控制依赖性时，会影响指令序列执行的并行度。为此，编译器和处理器会采用猜测（Speculation）执行来克服控制相关性对并行度的影响。以处理器的猜测执行为例，执行线程 B 的处理器可以提前读取并计算 a * a，然后把计算结果临时保存到一个名为重排序缓冲（reorder buffer ROB）的硬件缓存中。当接下来操作 3 的条件判断为真时，就把该计算结果写入变量 i 中。 从图中我们可以看出，猜测执行实质上对操作3和4做了重排序。重排序在这里破坏了多线程程序的语义！ 在单线程程序中，对存在控制依赖的操作重排序，不会改变执行结果（这也是 as-if-serial 语义允许对存在控制依赖的操作做重排序的原因）；但在多线程程序中，对存在控制依赖的操作重排序，可能会改变程序的执行结果。 顺序一致性顺序一致性内存模型 顺序一致性内存模型有两大特性： 一个线程中的所有操作必须按照程序的顺序来执行。 （不管程序是否同步）所有线程都只能看到一个单一的操作执行顺序。在顺序一致性内存模型中，每个操作都必须原子执行且立刻对所有线程可见。 顺序一致性内存模型为程序员提供的视图如下： 在概念上，顺序一致性模型有一个单一的全局内存，这个内存通过一个左右摆动的开关可以连接到任意一个线程，同时每一个线程必须按照程序的顺序来执行内存读/写操作。从上面的示意图我们可以看出，在任意时间点最多只能有一个线程可以连接到内存。当多个线程并发执行时，图中的开关装置能把所有线程的所有内存读/写操作串行化。 举个例子： 假设有两个线程 A 和 B 并发执行。其中 A 线程有三个操作，它们在程序中的顺序是：A1 -&gt; A2 -&gt; A3。B 线程也有三个操作，它们在程序中的顺序是：B1 -&gt; B2 -&gt; B3。 假设这两个线程使用监视器锁来正确同步：A 线程的三个操作执行后释放监视器锁，随后 B 线程获取同一个监视器锁。那么程序在顺序一致性模型中的执行效果将如下图所示： 现在我们再假设这两个线程没有做同步，下面是这个未同步程序在顺序一致性模型中的执行示意图： 未同步程序在顺序一致性模型中虽然整体执行顺序是无序的，但所有线程都只能看到一个一致的整体执行顺序。以上图为例，线程 A 和 B 看到的执行顺序都是：B1 -&gt; A1 -&gt; A2 -&gt; B2 -&gt; A3 -&gt; B3。之所以能得到这个保证是因为顺序一致性内存模型中的每个操作必须立即对任意线程可见。 但是，在 JMM 中就没有这个保证。未同步程序在 JMM 中不但整体的执行顺序是无序的，而且所有线程看到的操作执行顺序也可能不一致。比如，在当前线程把写过的数据缓存在本地内存中，在还没有刷新到主内存之前，这个写操作仅对当前线程可见；从其他线程的角度来观察，会认为这个写操作根本还没有被当前线程执行。只有当前线程把本地内存中写过的数据刷新到主内存之后，这个写操作才能对其他线程可见。在这种情况下，当前线程和其它线程看到的操作执行顺序将不一致。 ####### 同步程序的顺序一致性效果 下面我们对前面的示例程序用锁来同步，看看正确同步的程序如何具有顺序一致性。 请看下面的示例代码： 123456789101112131415class demo &#123; int a = 0; boolean flag = false; public synchronized void write() &#123; //获取锁 a = 1; flag = true; &#125; //释放锁 public synchronized void read() &#123; //获取锁 if(flag) &#123; int i = a; &#125; &#125; //释放锁&#125; 上面示例代码中，假设 A 线程执行 write() 方法后，B 线程执行 reade() 方法。这是一个正确同步的多线程程序。根据JMM规范，该程序的执行结果将与该程序在顺序一致性模型中的执行结果相同。下面是该程序在两个内存模型中的执行时序对比图： 在顺序一致性模型中，所有操作完全按程序的顺序执行。而在 JMM 中，临界区内的代码可以重排序（但 JMM 不允许临界区内的代码“逸出”到临界区之外，那样会破坏监视器的语义）。JMM 会在退出临界区和进入临界区这两个关键时间点做一些特别处理，使得线程在这两个时间点具有与顺序一致性模型相同的内存视图。虽然线程 A 在临界区内做了重排序，但由于监视器的互斥执行的特性，这里的线程 B 根本无法“观察”到线程 A 在临界区内的重排序。这种重排序既提高了执行效率，又没有改变程序的执行结果。 从这里我们可以看到 JMM 在具体实现上的基本方针：在不改变（正确同步的）程序执行结果的前提下，尽可能的为编译器和处理器的优化打开方便之门。 ####### 未同步程序的执行特性 未同步程序在 JMM 中的执行时，整体上是无序的，其执行结果无法预知。未同步程序在两个模型中的执行特性有下面几个差异： 顺序一致性模型保证单线程内的操作会按程序的顺序执行，而 JMM 不保证单线程内的操作会按程序的顺序执行（比如上面正确同步的多线程程序在临界区内的重排序）。 顺序一致性模型保证所有线程只能看到一致的操作执行顺序，而 JMM 不保证所有线程能看到一致的操作执行顺序。 JMM 不保证对 64 位的 long 型和 double 型变量的读/写操作具有原子性，而顺序一致性模型保证对所有的内存读/写操作都具有原子 。 第三个差异与处理器总线的工作机制密切相关。在计算机中，数据通过总线在处理器和内存之间传递。每次处理器和内存之间的数据传递都是通过总线事务来完成的。总线事务包括读事务和写事务。读事务从内存传送数据到处理器，写事务从处理器传递数据到内存，每个事务会读／写内存中一个或多个物理上连续的字。总线会同步试图并发使用总线的事务。在一个处理器执行总线事务期间，总线会禁止其它所有的处理器和 I／O 设备执行内存的读／写。 总线的工作机制： 如上图所示，假设处理器 A、B、和 C 同时向总线发起总线事务，这时总线仲裁会对竞争作出裁决，假设总线在仲裁后判定处理器 A 在竞争中获胜（总线仲裁会确保所有处理器都能公平的访问内存）。此时处理器 A 继续它的总线事务，而其它两个处理器则要等待处理器 A 的总线事务完成后才能开始再次执行内存访问。假设在处理器 A 执行总线事务期间（不管这个总线事务是读事务还是写事务），处理器 D 向总线发起了总线事务，此时处理器 D 的这个请求会被总线禁止。 总线的这些工作机制可以把所有处理器对内存的访问以串行化的方式来执行；在任意时间点，最多只能有一个处理器能访问内存。这个特性确保了单个总线事务之中的内存读/写操作具有原子性。 在一些 32 位的处理器上，如果要求对 64 位数据的写操作具有原子性，会有比较大的开销。为了照顾这种处理器，Java 语言规范鼓励但不强求 JVM 对 64 位的 long 型变量和 double 型变量的写具有原子性。当 JVM 在这种处理器上运行时，会把一个 64 位 long/ double 型变量的写操作拆分为两个 32 位的写操作来执行。这两个 32 位的写操作可能会被分配到不同的总线事务中执行，此时对这个 64 位变量的写将不具有原子性。 当单个内存操作不具有原子性，将可能会产生意想不到后果。请看下面示意图： 如上图所示，假设处理器 A 写一个 long 型变量，同时处理器 B 要读这个 long 型变量。处理器 A 中 64 位的写操作被拆分为两个 32 位的写操作，且这两个 32 位的写操作被分配到不同的写事务中执行。同时处理器 B 中 64 位的读操作被分配到单个的读事务中执行。当处理器 A 和 B 按上图的时序来执行时，处理器 B 将看到仅仅被处理器 A “写了一半“的无效值。 注意，在 JSR -133 之前的旧内存模型中，一个 64 位 long/ double 型变量的读/写操作可以被拆分为两个 32 位的读/写操作来执行。从 JSR -133 内存模型开始（即从JDK5开始），仅仅只允许把一个 64 位 long/ double 型变量的写操作拆分为两个 32 位的写操作来执行，任意的读操作在JSR -133中都必须具有原子性（即任意读操作必须要在单个读事务中执行）。 VolatileVolatile 特性举个例子： 123456789101112131415 1public class VolatileTest &#123; 2 volatile long a = 1L; // 使用 volatile 声明 64 位的 long 型 3 4 public void set(long l) &#123; 5 a = l; //单个 volatile 变量的写 6 &#125; 7 8 public long get() &#123; 9 return a; //单个 volatile 变量的读10 &#125;1112 public void getAndIncreament() &#123;13 a++; // 复合（多个） volatile 变量的读 /写14 &#125;15&#125; 假设有多个线程分别调用上面程序的三个方法，这个程序在语义上和下面程序等价： 1234567891011121314151617 1public class VolatileTest &#123; 2 long a = 1L; // 64 位的 long 型普通变量 3 4 public synchronized void set(long l) &#123; //对单个普通变量的写用同一个锁同步 5 a = l; 6 &#125; 7 8 public synchronized long get() &#123; //对单个普通变量的读用同一个锁同步 9 return a; 10 &#125;1112 public void getAndIncreament() &#123; //普通方法调用13 long temp = get(); //调用已同步的读方法14 temp += 1L; //普通写操作 15 set(temp); //调用已同步的写方法16 &#125;17&#125; 如上面示例程序所示，对一个 volatile 变量的单个读/写操作，与对一个普通变量的读/写操作使用同一个锁来同步，它们之间的执行效果相同。 锁的 happens-before 规则保证释放锁和获取锁的两个线程之间的内存可见性，这意味着对一个 volatile 变量的读，总是能看到（任意线程）对这个 volatile 变量最后的写入。 锁的语义决定了临界区代码的执行具有原子性。这意味着即使是 64 位的 long 型和 double 型变量，只要它是 volatile变量，对该变量的读写就将具有原子性。如果是多个 volatile 操作或类似于 volatile++ 这种复合操作，这些操作整体上不具有原子性。 简而言之，volatile 变量自身具有下列特性： 可见性。对一个 volatile 变量的读，总是能看到（任意线程）对这个 volatile 变量最后的写入。 原子性：对任意单个 volatile 变量的读/写具有原子性，但类似于 volatile++ 这种复合操作不具有原子性。 volatile 写-读的内存定义 当写一个 volatile 变量时，JMM 会把该线程对应的本地内存中的共享变量值刷新到主内存。 当读一个 volatile 变量时，JMM 会把该线程对应的本地内存置为无效。线程接下来将从主内存中读取共享变量。 假设上面的程序 flag 变量用 volatile 修饰 volatile 内存语义的实现下面是 JMM 针对编译器制定的 volatile 重排序规则表： 为了实现 volatile 的内存语义，编译器在生成字节码时，会在指令序列中插入内存屏障来禁止特定类型的处理器重排序。 下面是基于保守策略的 JMM 内存屏障插入策略： 在每个 volatile 写操作的前面插入一个 StoreStore 屏障。 在每个 volatile 写操作的后面插入一个 StoreLoad 屏障。 在每个 volatile 读操作的后面插入一个 LoadLoad 屏障。 在每个 volatile 读操作的后面插入一个 LoadStore 屏障。 下面是保守策略下，volatile 写操作 插入内存屏障后生成的指令序列示意图： 下面是在保守策略下，volatile 读操作 插入内存屏障后生成的指令序列示意图： 上述 volatile 写操作和 volatile 读操作的内存屏障插入策略非常保守。在实际执行时，只要不改变 volatile 写-读的内存语义，编译器可以根据具体情况省略不必要的屏障。 锁锁释放和获取的内存语义当线程释放锁时，JMM 会把该线程对应的本地内存中的共享变量刷新到主内存中。 当线程获取锁时，JMM 会把该线程对应的本地内存置为无效。从而使得被监视器保护的临界区代码必须要从主内存中去读取共享变量。 锁内存语义的实现借助 ReentrantLock 来讲解，PS：后面专门讲下这块（ReentrantLock、Synchronized、公平锁、非公平锁、AQS等），可以看看大明哥的博客：http://cmsblogs.com/?p=2210 concurrent 包的实现如果我们仔细分析 concurrent 包的源代码实现，会发现一个通用化的实现模式： 首先，声明共享变量为 volatile； 然后，使用 CAS 的原子条件更新来实现线程之间的同步； 同时，配合以 volatile 的读/写和 CAS 所具有的 volatile 读和写的内存语义来实现线程之间的通信。 AQS，非阻塞数据结构和原子变量类（java.util.concurrent.atomic 包中的类），这些 concurrent 包中的基础类都是使用这种模式来实现的，而 concurrent 包中的高层类又是依赖于这些基础类来实现的。从整体来看，concurrent 包的实现示意图如下： final对于 final 域，编译器和处理器要遵守两个重排序规则： 在构造函数内对一个 final 域的写入，与随后把这个被构造对象的引用赋值给一个引用变量，这两个操作之间不能重排序。 初次读一个包含 final 域的对象的引用，与随后初次读这个 final 域，这两个操作之间不能重排序。 写 final 域的重排序规则写 final 域的重排序规则禁止把 final 域的写重排序到构造函数之外。这个规则的实现包含下面2个方面： JMM 禁止编译器把 final 域的写重排序到构造函数之外。 编译器会在 final 域的写之后，构造函数 return 之前，插入一个 StoreStore 屏障。这个屏障禁止处理器把 final 域的写重排序到构造函数之外。 读 final 域的重排序规则在一个线程中，初次读对象引用与初次读该对象包含的 final 域，JMM 禁止处理器重排序这两个操作（注意，这个规则仅仅针对处理器）。编译器会在读 final 域操作的前面插入一个 LoadLoad 屏障。 final 域是引用类型对于引用类型，写 final 域的重排序规则对编译器和处理器增加了如下约束： 在构造函数内对一个 final 引用的对象的成员域的写入，与随后在构造函数外把这个被构造对象的引用赋值给一个引用变量，这两个操作之间不能重排序。]]></content>
  </entry>
  <entry>
    <title><![CDATA[线程和进程]]></title>
    <url>%2F2019%2F07%2F04%2F1%2F</url>
    <content type="text"><![CDATA[线程和进程概念进程（process）：是指具有已一定功能的独立程序，是系统资源分配的基本单位，在内存中有其完备的数据空间和代码空间，拥有完整的虚拟空间地址。一个进程所拥有的数据和变量只属于它自己。 线程（thread）：是进程内相对独立的可执行单元，所以也被称为轻量进程（lightweight processes）；是操作系统进行任务调度的基本单元。它与父进程的其它线程共享该进程所拥有的全部代码空间和全局变量，但拥有独立的堆栈（即局部变量对于线程来说是私有的）。 联系进程和线程都具有就绪、阻塞和运行三种基本状态。 一个进程至少拥有一个线程——主线程，也可以拥有多个线程；一个线程必须且仅有一个父进程。多个进程可以并发执行；一个线程可以创建和撤销另一个线程；同一个进程中的多个线程之间可以并发执行。 支援分配给进程，同一进程的所有线程共享该进程的所有资源 线程在执行过程中，需要协作同步。不同进程的线程间要利用消息通信的办法实现同步 处理机分给线程，即真正在处理机上运行的是线程 线程是指进程内的一个执行单元，也是进程内的可调度实体。 区别进程和线程的主要差别在于它们是不同的操作系统资源管理方式。当然也在系统开销方面有一些体现。 系统开销：在创建或撤消进程时，由于系统都要为之分配和回收资源，导致系统的开销明显大于创建或撤消线程时的开销 资源管理：进程有独立的地址空间，一个进程崩溃后，在保护模式下不会对其它进程产生影响，而线程只是一个进程中的不同执行路径。线程有自己的堆栈和局部变量，但线程之间没有单独的地址空间，一个线程死掉就等于整个进程死掉，（当一个线程向非法地址读取或者写入，无法确认这个操作是否会影响同一进程中的其它线程，所以只能是整个进程一起崩溃。）所以多进程的程序要比多线程的程序健壮，但在进程切换时，耗费资源较大，效率要差一些。但对于一些要求同时进行并且又要共享某些变量的并发操作，只能用线程，不能用进程。一个进程的开销大约是一个线程开销的30倍左右 优缺点线程和进程在使用上各有优缺点：线程执行开销小，但不利于资源的管理和保护；而进程正相反。同时，线程适合于在SMP机器上运行，而进程则可以跨机器迁移。 多进程单线程模型master进程管理worker进程： 接收来自外界的信号 向各worker进程发送信号 监控woker进程的运行状态 当woker进程退出后（异常情况下），会自动重新启动新的woker进程 这种模型提供了一种保护机制。当其中一个进程内部读取错误，master可以让错误进程重启。这使得你的服务器在表面上并没有感到“曾经崩溃”。 对于master，完全不涉及服务器的业务，使得错误进程能被安全隔离。 单进程多线程主线程负责监听客户端的连接请求，workers线程负责处理已经建立好的连接的读写等事件 一旦其中出现一个错误，整个进程都有可能挂掉。你当然可以为此进程编写一个“守护程序”来重启，但是重启期间，你的服务器是真的“挂掉了”。]]></content>
      <tags>
        <tag>线程</tag>
        <tag>进程</tag>
        <tag>线程和进程</tag>
      </tags>
  </entry>
</search>
